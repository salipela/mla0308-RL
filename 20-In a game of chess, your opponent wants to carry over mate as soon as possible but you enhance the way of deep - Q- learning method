import numpy as np

class ChessAgent:
    def __init__(self, state_size, action_size, learning_rate=0.001, epsilon_decay=0.995, epsilon_min=0.01):
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.epsilon = 1.0
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        self.Q = np.zeros((state_size, action_size))

    def choose_action(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.choice(self.action_size)  
        else:
            return np.argmax(self.Q[state]) 
    def update_q_table(self, state, action, reward, next_state, gamma):
        td_target = reward + gamma * np.max(self.Q[next_state])
        td_error = td_target - self.Q[state, action]
        self.Q[state, action] += self.learning_rate * td_error

    def decay_epsilon(self):
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay


state_size = 64 
action_size = 4096  
agent = ChessAgent(state_size, action_size)


num_episodes = 10
for episode in range(num_episodes):
    state = np.random.randint(2, size=state_size) 
    action = agent.choose_action(state)
    print("Episode:", episode, "Action chosen:", action)
    agent.decay_epsilon()
